# -*- coding: utf-8 -*-
"""fraud-detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16MNDPC3Q_3x8voGMe8eyT0wHAp-53TqN

IEEE Fraud Detection

## Part 1 - Fraudulent vs Non-Fraudulent Transaction
"""

# TODO: code and runtime results
from google.colab import files
files.upload()
!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle competitions download -c ieee-fraud-detection

"""Write your answer here"""

import pandas as pd
transactions = pd.read_csv('train_transaction.csv.zip')
transactions.shape
identities = pd.read_csv('train_identity.csv.zip')
identities.shape
identities.head()

fraudTransactions = transactions[transactions['isFraud'] == 1]
nonFraudTransactions = transactions[transactions['isFraud'] == 0]

fraudTransactions1 = fraudTransactions[['TransactionID','TransactionDT','TransactionAmt','ProductCD','card4','card6',
                                        'P_emaildomain','R_emaildomain','addr1','addr2','dist1','dist2']]
identitiesWithDTDI = identities[['TransactionID','DeviceType', 'DeviceInfo']]
fraudTransactions1WithIdentities = pd.merge(fraudTransactions1, identitiesWithDTDI, on='TransactionID')

nonFraudTransactions1 = nonFraudTransactions[['TransactionID','TransactionDT','TransactionAmt','ProductCD','card4','card6',
                                        'P_emaildomain','R_emaildomain','addr1','addr2','dist1','dist2']]
nonFraudTransactions1WithIdentities = pd.merge(nonFraudTransactions1, identitiesWithDTDI, on='TransactionID')
fraudTransactions1WithIdentities.describe()
fraudTransactions1WithIdentities.head()

fraudTransactions1.shape
nonFraudTransactions1.shape
nonFraudTransactions1WithIdentities.shape
fraudTransactions1WithIdentities['DeviceInfo'].value_counts()

"""It looks like the most number of fraudulent transactions have been done through a Windows PC, with 3121 transactions.

iOS comes a distant 2nd with around 1200 fraudulent transactions.
"""

fraudTransactions1['TransactionAmt'].value_counts()

"""Here we get a very **interesting insight**. The** maximum number** of fraudulent transactions has been done for a **product with a price tag of 117**. It means that it's **easy** to do a fraudulent transaction for a **particular product**.

Same is the case for **some other items** with a particular price tag.

We perform this operation **only on the Transaction table** as when we join the table with the Identity table, the number of rows **reduces to around 11k from 21k**, which I feel is a significant reduction in the sample size.
"""

fraudTransactions1['ProductCD'].value_counts()

fraudTransactions1['card4'].value_counts()

"""When we take it as a percentage

The most fraudulent transactions have been done through the **visa** card

On the second place is a distant mastercard with as many as half the transactions than that done through **visa**

All the other values are too few relative to **mastercard** and **visa**
"""

nonFraudTransactions1['card4'].value_counts()

"""When we look for the percentage, 

*   Visa and mastercard have similar percentage of fraudulent transactions out of overall transactions. The rate is around 3 %
*   8% of all Discover card transactions are fraudulent which is quite high for a reputed firm.

*   American Express stands at around 5%
"""

nonFraudTransactions1['card6'].value_counts()

"""## Part 2 - Transaction Frequency"""

trnFreq = transactions[['TransactionID', 'TransactionDT', 'addr2', 'isFraud']]
trnFreq.head()
trnFreq['addr2'].value_counts()
trnFreq['TransactionDT'].value_counts()

trnFreq87 = trnFreq[trnFreq['addr2'] == 87.0]
import matplotlib.pyplot as plt
 
import numpy as np
def make_hour_feature(df, tname='TransactionDT'):

    hours = df[tname] / (3600)        
    encoded_hours = np.floor(hours) % 24
    return encoded_hours

trnFreq87['hours'] = make_hour_feature(trnFreq87)
plt.plot(trnFreq87.groupby('hours').mean()['isFraud'], color='k')
ax = plt.gca()
ax2 = ax.twinx()
_ = ax2.hist(trnFreq87['hours'], alpha=0.3, bins=24)
ax.set_xlabel('Hours')
ax.set_ylabel('Fraction of fraudulent transactions')

ax2.set_ylabel('Number of transactions')

"""TransactionID

DeviceType (mobile/desktop/...)

DeviceInfo (Windows/MacOS/…)

TransactionDT (time delta from reference)

TransactionAmt (amount in USD)

ProductCD (product code - W/C/H/R/...)

card4 (card issuer)

card6 (debit/credit)

P_emaildomain (purchaser email)

R_emaildomain (recipient email)

addr1 / addr2 (billing region / billing country)

dist1 / dist2 (some form of distance - address, zip code, IP, phone, …)
"""

# TODO: code to generate the frequency graph

"""Write your answer here

## Part 3 - Product Code
"""

transactions.head()
transactionsProdAmt = transactions[['TransactionID', 'TransactionAmt', 'ProductCD']]
transactionsProdAmt.head()
transactionsProdAmt['TransactionAmt'].value_counts()
transactionsProdAmt.groupby('ProductCD')['TransactionAmt'].mean()

"""The product code R corresponds to the most expensive products with the mean of all the amounts coming out to be 168.31

The product code C corresponds to the least expensive products with the mean of 42.87

## Part 4 - Correlation Coefficient
"""

# TODO: code to calculate correlation coefficient
todAmt = transactions[['TransactionID', 'TransactionDT', 'addr2', 'TransactionAmt','ProductCD', 'isFraud', 'card6']]
todAmt.head()
todAmt['addr2'].value_counts()
todAmt['TransactionDT'].value_counts()


todAmt87 = todAmt[todAmt['addr2'] == 87.0]
import matplotlib.pyplot as plt
 
import numpy as np
def make_hour_feature(df, tname='TransactionDT'):
    hours = df[tname] / (3600)        
    encoded_hours = np.floor(hours) % 24
    return encoded_hours

todAmt87['hours'] = make_hour_feature(todAmt87)
plt.plot(todAmt87.groupby('hours')['TransactionAmt'].sum(), color='k')
ax = plt.gca()
ax2 = ax.twinx()
_ = ax2.hist(todAmt87['hours'], alpha=0.3, bins=24)
ax.set_xlabel('Encoded hour')
ax.set_ylabel('Product Amount')
ax2.set_ylabel('Number of transactions')

from scipy.stats import pearsonr
hour = []
for i in range(24):
  hour.append(i)
todAmt87SumGB = todAmt87.groupby('hours')['TransactionAmt'].sum()
todAmt87SumGB.describe()
list(todAmt87SumGB)
corr,_ = pearsonr(hour,todAmt87SumGB)
print('Pearsons correlation considering sum of Transaction Amount: %.3f' % corr)

plt.plot(todAmt87.groupby('hours')['TransactionAmt'].mean(), color='k')
axM = plt.gca()
axM2 = axM.twinx()
_ = axM2.hist(todAmt87['hours'], alpha=0.3, bins=24)
axM.set_xlabel('Encoded hour')
axM.set_ylabel('Product Amount')
axM2.set_ylabel('Number of transactions')

todAmt87MeanGB = todAmt87.groupby('hours')['TransactionAmt'].mean()
todAmt87MeanGB.describe()
list(todAmt87MeanGB)
corr,_ = pearsonr(hour,todAmt87MeanGB)
print('Pearsons correlation considering mean of Transaction Amount: %.3f' % corr)

"""Write your answer here

## Part 5 - Interesting Plot
"""

# TODO: code to generate the plot here.

"""The plots of time of day vs product is interesting and it's getting a correlation co-efficient of 0.65

Also, the plost of card6 variable vs isFraud is interesting.
The percentage of number of fraudulent transactions for credit card is almost double than that of debit card!!!!
Beware credit card holders!

## Part 6 - Prediction Model

Write your answer here
"""

# TODO: code for your final model
import seaborn as sns
sns.countplot(x='isFraud',hue = 'card6', data=transactions)
transactionAmtByFraudness = transactions.groupby('isFraud')['TransactionAmt'].sum()
list(transactionAmtByFraudness)

transactions.isnull().sum()
transactions.groupby('isFraud').mean()

from sklearn.model_selection import train_test_split
cleanup_Prod = {"ProductCD":     {"C": 4, "H": 2, "S": 3, "R":4, "W":5}}
cleanup_card6 = {"card6": {"debit":1, "credit":2, "debit or credit":3, "charge card": 4}}
todAmtReplaced = todAmt87.replace(cleanup_Prod)
feature_cols = ['TransactionAmt', 'hours', 'ProductCD']

X = todAmtReplaced[feature_cols] # Features
y = todAmtReplaced.isFraud # Target variable
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)

#split dataset in features and target variable

from sklearn.linear_model import LogisticRegression

# instantiate the model (using the default parameters)
logreg = LogisticRegression()

# fit the model with data
logreg.fit(X_train,y_train)

todAmtReplaced['card6'].fillna(0, inplace=True)
todAmtReplaced['card6'].isna().sum()
transactions['DeviceType'].value_counts()

transactions_test = pd.read_csv('test_transaction.csv.zip')
transactions_test.replace(cleanup_Prod, inplace=True)

transactions_test['hours'] = make_hour_feature(transactions_test)
cleanup_Prod = {"ProductCD":     {"C": 4, "H": 2, "S": 3, "R":4, "W":5}}

transactions_test.head()
transactions_test_columns = transactions_test[['TransactionAmt', 'hours', 'ProductCD']]
transactions_test_columns.head()

y_pred=logreg.predict(transactions_test_columns)

sampleSubmission = pd.read_csv("sample_submission.csv.zip")
sampleSubmission['isFraud'] = y_pred
sampleSubmission.head()

sampleSubmission.to_csv("sampleSubmission1.csv")
sampleSubmission.head()

"""## Part 7 - Final Result

**Score**: 0.88
"""